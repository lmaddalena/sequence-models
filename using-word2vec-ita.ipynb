{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_word2vec_matrix(word2vec_file):\n",
    "    with open(word2vec_file, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "\n",
    "        line = f.readline().strip().split()\n",
    "        m = line[0] # number of rows\n",
    "        n = line[1] # number of cols\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\n",
    "    return words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words:  2469122\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'set' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ce581117337b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_vec_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_word2vec_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../../nlp/model.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'words: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word_vec_[0]: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_vec_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "words, word_to_vec_map = read_word2vec_matrix('../../../nlp/model.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words[18623]:  proguanile\n",
      "word_to_vec_map[words[18623]:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5.19170e-02, -2.16962e-01,  2.91658e-01, -5.59510e-02,\n",
       "        3.82250e-02,  1.33154e-01, -3.43920e-01,  1.54411e-01,\n",
       "        9.16760e-02,  7.03040e-02,  9.55600e-03,  4.36170e-02,\n",
       "       -3.87940e-02, -1.35658e-01,  1.64581e-01, -1.01549e-01,\n",
       "        1.24315e-01, -3.70790e-02,  1.50376e-01,  1.55852e-01,\n",
       "       -1.54380e-01,  5.34490e-02,  8.69620e-02,  3.05327e-01,\n",
       "        2.22060e-01, -1.27903e-01, -7.02760e-02,  2.80031e-01,\n",
       "        9.70360e-02, -1.13833e-01, -2.17892e-01,  1.59282e-01,\n",
       "       -4.14302e-01,  1.82430e-01,  3.77200e-02,  4.09520e-02,\n",
       "        2.37330e-02, -1.61899e-01,  6.48000e-02,  6.82770e-02,\n",
       "        3.05240e-02,  1.21533e-01, -1.89350e-02,  4.70600e-02,\n",
       "       -2.34020e-02,  1.68790e-01,  9.46610e-02,  1.36476e-01,\n",
       "       -2.09627e-01, -1.43936e-01,  2.51430e-01, -1.79624e-01,\n",
       "       -1.23201e-01,  1.03637e-01, -5.14990e-02,  9.56310e-02,\n",
       "        5.80380e-02,  8.55280e-02, -8.73440e-02, -1.75922e-01,\n",
       "        1.93957e-01,  2.84770e-02, -1.14581e-01,  1.38230e-02,\n",
       "        2.87910e-02,  3.02621e-01,  1.40999e-01, -7.01090e-02,\n",
       "        4.11208e-01,  1.43007e-01,  7.80000e-05, -1.94982e-01,\n",
       "       -6.63770e-02, -3.91198e-01, -1.75740e-01,  4.38360e-02,\n",
       "       -2.40322e-01,  1.63901e-01, -9.61470e-02,  1.31890e-02,\n",
       "       -1.71820e-02,  1.87513e-01,  2.35150e-02, -2.34800e-01,\n",
       "       -4.41920e-02,  2.29208e-01,  1.07580e-02, -5.11850e-02,\n",
       "        2.68808e-01, -4.00950e-02, -2.37400e-03,  2.28800e-02,\n",
       "        6.26790e-02,  2.75561e-01,  1.27388e-01,  1.17578e-01,\n",
       "        6.89350e-02,  3.71149e-01, -9.33320e-02,  1.08438e-01])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(words)\n",
    "print(\"words[18623]: \", words[18623])\n",
    "print(\"word_to_vec_map[words[18623]:\")\n",
    "word_to_vec_map[words[18623]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similariy between u and v\n",
    "        \n",
    "    Arguments:\n",
    "        u -- a word vector of shape (n,)          \n",
    "        v -- a word vector of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
    "    \"\"\"\n",
    "    \n",
    "    distance = 0.0\n",
    "   \n",
    "    dot = np.dot(u, v)\n",
    "\n",
    "    # Compute the L2 norm of u (≈1 line)\n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "    \n",
    "    # Compute the L2 norm of v (≈1 line)\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "\n",
    "    # Compute the cosine similarity defined by formula (1) (≈1 line)\n",
    "    cosine_similarity = dot / np.dot(norm_u, norm_v)\n",
    "\n",
    "   \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(padre, madre) =  0.7659759663297602\n",
      "cosine_similarity(palla, coccodrillo) =  0.3832868493247648\n",
      "cosine_similarity(francia - parigi, roma - italia) =  -0.39235044171214056\n"
     ]
    }
   ],
   "source": [
    "father = word_to_vec_map[\"padre\"]\n",
    "mother = word_to_vec_map[\"madre\"]\n",
    "ball = word_to_vec_map[\"palla\"]\n",
    "crocodile = word_to_vec_map[\"coccodrillo\"]\n",
    "france = word_to_vec_map[\"francia\"]\n",
    "italy = word_to_vec_map[\"italia\"]\n",
    "paris = word_to_vec_map[\"parigi\"]\n",
    "rome = word_to_vec_map[\"rooma\"]\n",
    "\n",
    "print(\"cosine_similarity(padre, madre) = \", cosine_similarity(father, mother))\n",
    "print(\"cosine_similarity(palla, coccodrillo) = \",cosine_similarity(ball, crocodile))\n",
    "print(\"cosine_similarity(francia - parigi, roma - italia) = \",cosine_similarity(france - paris, rome - italy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Performs the word analogy task as explained above: a is to b as c is to ____. \n",
    "    \n",
    "    Arguments:\n",
    "    word_a -- a word, string\n",
    "    word_b -- a word, string\n",
    "    word_c -- a word, string\n",
    "    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n",
    "    \n",
    "    Returns:\n",
    "    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert words to lower case\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    # Get the word embeddings v_a, v_b and v_c (≈1-3 lines)\n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
    "        \n",
    "    words = word_to_vec_map.keys()\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
    "\n",
    "    # loop over the whole word vector set\n",
    "    for w in words:        \n",
    "        # to avoid best_word being one of the input words, pass on them.\n",
    "        if w in [word_a, word_b, word_c] :\n",
    "            continue\n",
    "        \n",
    "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n",
    "        cosine_sim = cosine_similarity((e_b - e_a), (word_to_vec_map[w] - e_c))\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italia -> italiano :: spagna -> spagnolo\n",
      "india -> delhi :: giappone -> osaka\n",
      "uomo -> donna :: ragazzo -> ragazza\n",
      "piccolo -> minuscolo :: grande -> minuscola\n"
     ]
    }
   ],
   "source": [
    "triads_to_try = [('italia', 'italiano', 'spagna'), ('india', 'delhi', 'giappone'), ('uomo', 'donna', 'ragazzo'), ('piccolo', 'minuscolo', 'grande')]\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, words_analogy(*triad,word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mela -> frutta :: carota -> verdure\n"
     ]
    }
   ],
   "source": [
    "triads_to_try = [('mela', 'frutta', 'carota')]\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, words_analogy(*triad,word_to_vec_map)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c48d435e47ba5d5a932a530ddeb248c9baa0081132f1819dcaeedbf0fcd2e46"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('venvML': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
